{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Author: Riya Chougule"
      ],
      "metadata": {
        "id": "5xjr_QEArsh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATE: 13th February, 2024"
      ],
      "metadata": {
        "id": "7GiNT_2ur8BJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description: This Python notebook presents a Retrieval Augmented Generation chatbot based on FDA (Food and Drug Administration) drug guidelines. With the FDA regularly publishing thousands of guidelines annually, including over 2100 documents solely dedicated to drugs, our chatbot streamlines user interactions by eliminating the need to manually explore extensive documentation. Leveraging input PDF data, the chatbot swiftly provides accurate responses to user queries based on FDA Drug guidelines. Additionally, the notebook integrates a user-friendly interface via Gradio, enhancing accessibility and usability."
      ],
      "metadata": {
        "id": "QnFFZRLGsGPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install PyPDF\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "znKMn2CElr44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAqgYBC8le4T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain import hub\n",
        "import PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set up environment variables if needed\n",
        "os.environ['OPENAI_API_KEY'] = <YOUR OPENAI KEY>\n",
        "os.environ['LANGCHAIN_API_KEY'] = <YOUR LANGCHAIN KEY>\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.langchain.plus'\n",
        "os.environ['LANGCHAIN_PROJECT'] = 'Explore Evaluating index using LLM'\n",
        "\n",
        "# Define functions for loading API keys and PDF processing\n",
        "def load_api_key(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            api_key = file.read().strip()\n",
        "            return api_key\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: File not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "Wk8zHLj1lt2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf_text(path):\n",
        "    text = \"\"\n",
        "    with open(path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        num_pages = len(reader.pages)\n",
        "        for page_num in range(num_pages):\n",
        "            page = reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Load API keys\n",
        "\n",
        "file_path_openai = \"/content/drive/MyDrive/Data/api_key.txt\"\n",
        "file_path_langchain = \"/content/drive/MyDrive/Data/langchain-api.txt\"\n",
        "openai_api_key = load_api_key(file_path_openai)\n",
        "langchain_api_key = load_api_key(file_path_langchain)\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key\n",
        "\n",
        "# Define the directory containing PDF files\n",
        "pdf_directory = \"/content/drive/MyDrive/Data/PDFs\"\n"
      ],
      "metadata": {
        "id": "tTULKtzNmVNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the vector store\n",
        "vectorstore = Chroma()\n",
        "\n",
        "# Loop through the PDF files in the directory\n",
        "for filename in os.listdir(pdf_directory):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        file_path = os.path.join(pdf_directory, filename)\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        data = loader.load()\n",
        "\n",
        "        # Split the PDF text into chunks\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "        splits = text_splitter.split_documents(data)\n",
        "\n",
        "        vectorstore = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "QAyTaJjDmtCG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca9d13a-b461-41e0-eaea-269e8101bd24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RAG framework components\n",
        "retriever = vectorstore.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Post-processing function\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Define the RAG chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Function to generate response\n",
        "def generate_response(Question):\n",
        "    response = rag_chain.invoke(Question)\n",
        "    return response"
      ],
      "metadata": {
        "id": "YL7z3F2imzfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USER INTERFACE\n",
        "import gradio as gr\n",
        "def generate_response(Question):\n",
        "    # Define basic greetings and responses\n",
        "    greetings = [\"hi\", \"hello\", \"hey!\", \"Hola!\", \"Hey\",\"Hello\"]\n",
        "    thank_you = [\"thank you\", \"thanks\", \"thanks!\"]\n",
        "\n",
        "    # Convert the input question to lowercase for case-insensitive matching\n",
        "    question = Question.lower()\n",
        "\n",
        "    # Check if the input question is a basic greeting\n",
        "    if question in greetings:\n",
        "        response = \"Hello! How can I assist you today?\"\n",
        "    # Check if the input question is an expression of gratitude\n",
        "    elif question in thank_you:\n",
        "        response = \"You're welcome! If you have any more questions or if there's anything else I can assist you with, please don't hesitate to let me know.\"\n",
        "    else:\n",
        "        # Call your machine learning model here to generate a response based on the query\n",
        "        response = rag_chain.invoke(question)\n",
        "    return response\n",
        "\n",
        "# Create the Gradio interface with custom CSS\n",
        "iface = gr.Interface(\n",
        "    fn=generate_response,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"<div style='padding-left: 520px; font-family: Zefani; font-size: 40px;'>RegulEase</div>\",\n",
        "    description=\"<div style='padding-left: 470px; font-size: x-large; font-family: Zefani; padding-left: 352px; '>Navagating FDA Compliance: Your Fast-Track Guide</div> <br><img src='https://today.uconn.edu/wp-content/uploads/2021/11/AdobeStock_333930165-scaled.jpeg' style='width: 850px; padding-left: 365px; height: 170px;'>\",\n",
        "    examples=[\n",
        "        [\"As a manufacturer are there any specific recommendations I need to follow for multi-site manufacturing for CAR-T cell therapy?\"],\n",
        "        [\"Are there any alternative approaches or methodologies recognized by regulatory authorities for demonstrating bioequivalence, particularly for complex or modified-release formulations?\"],\n",
        "        [\"While developing CART cell therapy for pediatric patients, what considerations I should take into account?\"]\n",
        "    ],\n",
        "    theme=\"compact\",    # Use the compact theme for buttons\n",
        "    allow_flagging=True,  # Allow flagging examples\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch(server_port=4409)"
      ],
      "metadata": {
        "id": "3eMUP39LnG0Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "outputId": "0f47dbd9-011c-4029-9752-8821b486a455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:555: UserWarning: Cannot load compact. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/compact (Request ID: Root=1-65e9f4d6-325878dc0bc5e79271011561;9115ae5f-19ca-4e4f-a50d-1d10d52fdd13)\n",
            "\n",
            "Sorry, we can't find the page you are looking for.\n",
            "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/interface.py:362: UserWarning: The `allow_flagging` parameter in `Interface` nowtakes a string value ('auto', 'manual', or 'never'), not a boolean. Setting parameter to: 'manual'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://dc54934ed9caf9b134.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dc54934ed9caf9b134.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}